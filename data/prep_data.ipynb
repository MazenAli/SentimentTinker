{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./raw/sentiment.csv\"\n",
    "df = pd.read_csv(path, quotechar='\"', on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    print(df.values[k][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "def preprocess_sentence(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?!,多]+\", \" \", w)\n",
    "\n",
    "        w = w.strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = df['SentimentText'].values[23]\n",
    "print(w)\n",
    "w = unicode_to_ascii(w.lower().strip())\n",
    "w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "w = re.sub(r'[\" \"]+', \" \", w)\n",
    "w = re.sub(r\"[^a-zA-Z?!,多]+\", \" \", w)\n",
    "w = w.strip()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProcessedText'] = df['SentimentText'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load training/testing data\n",
    "with open('train_valid_test_data.pkl', 'rb') as f:\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = pickle.load(f)\n",
    "\n",
    "# Load tokenizer\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1262889, 100)\n",
      "(157861, 100)\n",
      "(157862, 100)\n",
      "====\n",
      "[1, 1, 0, 1, 0, 0, 1, 1, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 1, 1, 1, 0]\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(\"====\")\n",
    "print(y_train[:10])\n",
    "print(y_valid[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m features \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m labels \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m X_train, X_valid, X_test, y_train, y_valid, y_test \u001b[39m=\u001b[39m train_test_split(features, labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(X_train\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(X_valid\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample dataframe\n",
    "df = pd.DataFrame({\n",
    "    'features': ['feature'+str(i) for i in range(10)],\n",
    "    'labels': ['label'+str(i) for i in range(10)],\n",
    "})\n",
    "\n",
    "features = df['features']\n",
    "labels = df['labels']\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.1\n",
    "features = df['ProcessedText']\n",
    "labels = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=42, stratify=labels)\n",
    "X_train, X_test, y_train, y_test = X_train.tolist(), X_test.tolist(), y_train.tolist(), y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Tokenize your text data\n",
    "num_words = 1000\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='', oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad your sequences\n",
    "max_sequence_length = 100\n",
    "X_train_padded = pad_sequences(sequences_train, maxlen=max_sequence_length, padding=\"post\")\n",
    "X_test_padded = pad_sequences(sequences_test, maxlen=max_sequence_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 18:02:41.951001: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-04 18:02:41.953370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-04 18:02:41.954852: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "embedding_dim = 64\n",
    "num_cells = 10\n",
    "num_classes = 2\n",
    "num_words=1000\n",
    "max_sequence_length=100\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(num_cells))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1420750, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_padded))\n",
    "print(X_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
       "array([[0.49729493, 0.5084913 ],\n",
       "       [0.49729493, 0.5084913 ],\n",
       "       [0.49729493, 0.5084913 ],\n",
       "       [0.49729493, 0.5084913 ],\n",
       "       [0.49729493, 0.5084913 ],\n",
       "       [0.49729493, 0.5084913 ],\n",
       "       [0.49729493, 0.5084913 ],\n",
       "       [0.49729493, 0.5084913 ],\n",
       "       [0.4972949 , 0.5084913 ],\n",
       "       [0.4972949 , 0.5084913 ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = model(X_train_padded[0:10])\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"Original Text:\", X_train[i])\n",
    "    print(\"Sequence:\", sequences_train[i])\n",
    "    print(type(X_train[i]))\n",
    "    print(len(X_train[i]))\n",
    "    print(len(sequences_train[i]))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Joining the tokens into sentences\n",
    "X_train_joined = [' '.join(tokens) for tokens in X_train]\n",
    "X_test_joined = [' '.join(tokens) for tokens in X_test]\n",
    "\n",
    "X_train_joined = [sentence.replace('& quot ;', '') for sentence in X_train_joined]\n",
    "X_test_joined = [sentence.replace('& quot ;', '') for sentence in X_test_joined]\n",
    "\n",
    "# Create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Tokenize and build vocab\n",
    "vectorizer.fit(X_train_joined)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document\n",
    "X_train_tfidf = vectorizer.transform(X_train_joined)\n",
    "X_test_tfidf = vectorizer.transform(X_test_joined)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_joined[:10])\n",
    "print(X_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train_tfidf))\n",
    "print(type(y_train))\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "sparse.save_npz(\"./sentiment/X_train_tfidf.npz\", X_train_tfidf)\n",
    "sparse.save_npz(\"./sentiment/X_test_tfidf.npz\", X_test_tfidf)\n",
    "sparse.save_npz(\"./sentiment/y_train.npz\", y_train)\n",
    "sparse.save_npz(\"./sentiment/y_test.npz\", y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimenttinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
