{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define model\n",
    "inputs = Input(shape=(5, 2))\n",
    "lstm = tf.keras.layers.LSTM(4)\n",
    "softmax = tf.keras.layers.Softmax()\n",
    "output = softmax(lstm(inputs))\n",
    "model = Model(inputs=inputs, outputs=output, name=\"my_lstm\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"acccuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load training/testing data\n",
    "with open('../data/train_valid_test_data.pkl', 'rb') as f:\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = pickle.load(f)\n",
    "\n",
    "# Load tokenizer\n",
    "with open('../data/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from random_search/sentiment_analysis/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# We'll keep the model definition in a function\n",
    "# The function takes an argument hp from which you can sample hyperparameters\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=1000,\n",
    "        output_dim=hp.Int('output_dim', min_value=32, max_value=128, step=32),\n",
    "        input_length=100)\n",
    "    )\n",
    "    model.add(LSTM(hp.Int('lstm_units', min_value=32, max_value=256, step=32)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Add hyperparameters for the optimizer\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='random_search',\n",
    "    project_name='sentiment_analysis'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 3\n",
      "output_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "lstm_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'rmsprop'], 'ordered': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 19:24:41.241681: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-04 19:24:41.243107: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-04 19:24:41.244356: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner\n",
    "model = build_model(keras_tuner.HyperParameters())\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.all((X_train >= 0) & (X_train <= 998)))\n",
    "print(np.all((X_valid >= 0) & (X_valid <= 999)))\n",
    "print(set(y_train))\n",
    "print(set(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train)) # should be <class 'numpy.ndarray'>\n",
    "print(type(X_valid)) # should be <class 'numpy.ndarray'>\n",
    "print(type(y_train)) # should be <class 'numpy.ndarray'> or <class 'list'>\n",
    "print(type(y_valid)) # should be <class 'numpy.ndarray'> or <class 'list'>\n",
    "print(X_train.shape) \n",
    "print(y_train.shape) \n",
    "print(X_valid.shape) \n",
    "print(y_valid.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "X = tf.random.normal((10, 5, 2))\n",
    "pred = model(X)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()  # list of numpy arrays\n",
    "    print(weights)\n",
    "    if len(weights)>0:\n",
    "        for w in weights:\n",
    "            print(w.shape)\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()  # list of numpy arrays\n",
    "    for weight in weights:\n",
    "        plt.hist(weight.flatten())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "x_sample = tf.random.normal((3, 5, 2))\n",
    "y_sample = [1, 2, 2]\n",
    "\n",
    "# suppose x_sample and y_sample are your input features and labels\n",
    "y_pred = model.predict(x_sample)\n",
    "\n",
    "loss = loss_fn(y_sample, y_pred)\n",
    "print('Loss:', loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "num_samples = 100\n",
    "seq_length = 10\n",
    "num_features = 2\n",
    "num_classes = 5\n",
    "train_ratio = 0.8\n",
    "num_lstm_cells = 10\n",
    "train_split = int(train_ratio*num_samples)\n",
    "\n",
    "# Generate sequential data where each value depends on the previous one\n",
    "X = np.empty((num_samples, seq_length, num_features))\n",
    "for i in range(num_samples):\n",
    "    for j in range(seq_length):\n",
    "        for k in range(num_features):\n",
    "            if j > 0:\n",
    "                X[i, j, k] = X[i, j - 1, k] * 0.9 + np.random.normal(0, 0.1)\n",
    "            else:\n",
    "                X[i, j, k] = np.random.normal(0, 0.1)\n",
    "\n",
    "# Generate labels that are a function of the last value in each sequence\n",
    "y = np.sum(X[:, -1, :], axis=1)\n",
    "y = np.round(y).astype(int)  # convert to integers for classification\n",
    "y = np.clip(y, 0, num_classes - 1)  # clip to the range 0 to num_classes - 1\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "X_train = X[:train_split]\n",
    "y_train = y[:train_split]\n",
    "\n",
    "X_test = X[train_split:]\n",
    "y_test = y[train_split:]\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(num_lstm_cells, input_shape=(seq_length, num_features)),  # LSTM layer\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "hist = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimenttinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
